{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e00253",
   "metadata": {},
   "source": [
    "# ðŸ¤– FraudLens Model Training Notebook\n",
    "This notebook loads scraped Facebook page data, processes it, and trains a logistic regression fraud classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac150c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'â€¦ see more', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess(df):\n",
    "    df['About Cleaned'] = df['About'].fillna(\"\").apply(clean_text)\n",
    "    df['Recommendation Sentiment'] = df['Recommendation'].fillna(\"\").apply(lambda x: 1 if \"recommend\" in x.lower() else 0)\n",
    "    df['Cleaned Content'] = df['Post Content'].fillna(\"\").apply(clean_text)\n",
    "    df['Post Length'] = df['Cleaned Content'].apply(len)\n",
    "    df['Num Comments'] = df['Comments'].apply(len)\n",
    "    df['Total Reactions'] = df['Reactions'].apply(lambda x: sum(x.values()) if isinstance(x, dict) else 0)\n",
    "    df['Angry Ratio'] = df['Reactions'].apply(lambda x: x.get('Angry', 0) / sum(x.values()) if sum(x.values()) > 0 else 0)\n",
    "    df['Sad Ratio'] = df['Reactions'].apply(lambda x: x.get('Sad', 0) / sum(x.values()) if sum(x.values()) > 0 else 0)\n",
    "    df['Haha Ratio'] = df['Reactions'].apply(lambda x: x.get('Haha', 0) / sum(x.values()) if sum(x.values()) > 0 else 0)\n",
    "    df['Love Ratio'] = df['Reactions'].apply(lambda x: x.get('Love', 0) / sum(x.values()) if sum(x.values()) > 0 else 0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")  # Adjust path if needed\n",
    "combined_df = []\n",
    "reviews_corpus = []\n",
    "\n",
    "for file in data_dir.glob(\"*.json\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        posts = pd.DataFrame(data[\"Posts\"])\n",
    "        posts[\"About\"] = data.get(\"About\", \"\")\n",
    "        posts[\"Recommendation\"] = data.get(\"Recommendation\", \"\")\n",
    "        posts[\"Reviews\"] = [data.get(\"Reviews\", [])] * len(posts)\n",
    "        combined_df.append(posts)\n",
    "        reviews_corpus.extend([r[\"Review\"] for r in data.get(\"Reviews\", []) if r.get(\"Review\")])\n",
    "\n",
    "df = pd.concat(combined_df, ignore_index=True)\n",
    "df = preprocess(df)\n",
    "print(\"âœ… Loaded and preprocessed:\", len(df), \"posts\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text\n",
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "X_text = tfidf.fit_transform(reviews_corpus + df['Cleaned Content'].tolist())\n",
    "y_dummy = [0] * len(reviews_corpus) + [1] * len(df)\n",
    "clf_text = LogisticRegression().fit(X_text, y_dummy)\n",
    "df['Text_Prob'] = clf_text.predict_proba(tfidf.transform(df['Cleaned Content']))[:, 1]\n",
    "\n",
    "# Anomaly detection\n",
    "features = ['Post Length', 'Num Comments', 'Total Reactions', 'Angry Ratio', 'Sad Ratio', 'Haha Ratio', 'Love Ratio', 'Recommendation Sentiment']\n",
    "X_behavior = df[features].fillna(0)\n",
    "anomaly_model = IsolationForest(contamination=0.25, random_state=42)\n",
    "df['Anomaly_Score'] = -anomaly_model.fit(X_behavior).decision_function(X_behavior)\n",
    "\n",
    "# Simulate blockchain trust\n",
    "np.random.seed(42)\n",
    "df['Trust_Score'] = np.random.uniform(0.5, 1.0, len(df))\n",
    "\n",
    "# Final score fusion\n",
    "df['FraudLens_Score'] = (\n",
    "    0.35 * df['Text_Prob'] +\n",
    "    0.35 * df['Anomaly_Score'] +\n",
    "    0.2 * (1 - df['Trust_Score']) +\n",
    "    0.1 * df['Recommendation Sentiment']\n",
    ")\n",
    "df['Fraud_Prediction'] = df['FraudLens_Score'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "df[['Post Content', 'FraudLens_Score', 'Fraud_Prediction']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fraudlens_training_predictions.csv\", index=False)\n",
    "print(\"âœ… Predictions saved to 'fraudlens_training_predictions.csv'\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
